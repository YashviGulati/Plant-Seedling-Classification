{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path=\"C:\\\\Users\\\\asd\\\\Desktop\\\\Plant\\\\train\\\\Train\\\\\"\n",
    "#test_path=\"C:\\\\Users\\\\Yashvi Gulati\\\\Documents\\\\Pucho\\\\test\"\n",
    "imgsize=32\n",
    "c=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_array=np.zeros(12, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_function(img):\n",
    "    file_name=img.split('.')[-3]\n",
    "    hot_array=np.zeros(12, dtype=int)\n",
    "    q=int(file_name[5]+file_name[6])-1\n",
    "    hot_array[q]=1\n",
    "    return hot_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createbatch(batchsize,j,array):\n",
    "    batch=array[j:j+batchsize]\n",
    "    imgarray=[]\n",
    "    labelarray=[]\n",
    "    for k in batch:\n",
    "        filepath=os.path.join(train_path,k)\n",
    "        name=label_function(os.path.basename(filepath))\n",
    "        labelarray.append(name)\n",
    "        img=cv2.imread(filepath)\n",
    "        img= cv2.resize(img, (imgsize,imgsize))\n",
    "        imgarray.append(img)\n",
    "    \n",
    "    train_data=np.array(imgarray)\n",
    "    train_label=np.array(labelarray)\n",
    "    \n",
    "    return train_data, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, imgsize,imgsize,c])\n",
    "y_= tf.placeholder(tf.float32, shape=[None,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 3, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "h_conv1 = tf.nn.relu((conv2d(x, W_conv1) + b_conv1)) #20X128X128X32\n",
    "h_pool1 = max_pool_2x2(h_conv1) #20X64X64X32\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) #20X64X64X64\n",
    "h_pool2 = max_pool_2x2(h_conv2) #20X32X32X64\n",
    "\n",
    "W_conv3 = weight_variable([5, 5, 32, 128])\n",
    "b_conv3 = bias_variable([128])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3) #20X32X32X128\n",
    "h_pool3 = max_pool_2x2(h_conv3) #20X16X16X128\n",
    "\n",
    "W_fc1 = weight_variable([8 * 8 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool3, [-1, 8*8*64]) #converting into 2d\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([1024, 12])\n",
    "b_fc2 = bias_variable([12])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(1e-2).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, training accuracy 0.133895\n",
      "epoch 1, training accuracy 0.159368\n",
      "epoch 2, training accuracy 0.175158\n",
      "epoch 3, training accuracy 0.199158\n",
      "epoch 4, training accuracy 0.227158\n",
      "epoch 5, training accuracy 0.251158\n",
      "epoch 6, training accuracy 0.261684\n",
      "epoch 7, training accuracy 0.280842\n",
      "epoch 8, training accuracy 0.286737\n",
      "epoch 9, training accuracy 0.277895\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:#declaring tf.Session as sess\n",
    "    sess.run(tf.global_variables_initializer())#initializing all the variables we used\n",
    "    for j in range(10) :#epochs \n",
    "        imList=shuffle(os.listdir(train_path))\n",
    "        for i in range(24):#number of iterations we are using\n",
    "            batchData,batchLabel = createbatch(190,i*190,imList)\n",
    "            train_step.run(feed_dict={x: batchData, y_: batchLabel, keep_prob: 0.7})#dropout probabilty during training is kept 0.5\n",
    "        train_accuracy = 0\n",
    "        valid_accuracy = 0\n",
    "        for i in range(24):#number of iterations we are using\n",
    "            batchData,batchLabel = createbatch(190,i*190,imList)\n",
    "            train_accuracy += accuracy.eval(feed_dict={x: batchData, y_: batchLabel, keep_prob: 1})#training accuracy calculated with no dropout probabilty\n",
    "        print('epoch %d, training accuracy %g' % (j, train_accuracy / 24))\n",
    "        valData,valLabel=createbatch(190,24*190,imList)\n",
    "        valid_accuracy += accuracy.eval(feed_dict={x: valData, y_: valLabel, keep_prob: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Accuracy is 28.9473685126 %\n",
      "Final Training Accuracy is 28.9473682642 %\n"
     ]
    }
   ],
   "source": [
    "print('Final Training Accuracy is', train_accuracy*(100/24), '%')\n",
    "print('Final Training Accuracy is', valid_accuracy*100, '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
